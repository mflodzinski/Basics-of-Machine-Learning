{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe860566",
   "metadata": {},
   "source": [
    "# Wstęp do sztucznej inteligencji\n",
    "Ćwiczenie 4 - Drzewa Decyzyjne  \n",
    "Maciej Łodziński  \n",
    "\n",
    "Celem ćwiczenia była implementacja drzewa decyzyjnego ID3 z uwzględnieniem maksymalnej głębokości przeszukiwania.   \n",
    "Zbiorem danych, na którym trzeba było testować wyliczony model są dane pacjentów cierpiących lub nie na chorobę   \n",
    "serca znajdujące się w pliku `cardio_train.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5464311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from abc import ABC, abstractmethod\n",
    "from random import choices\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "839b7131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean split data into train, validation and test data -> 6:2:2\n",
    "def split_data(filename, deli):\n",
    "    data = pd.read_csv(filename, sep=deli)\n",
    "    data = data.dropna(axis=0)\n",
    "    data = data.head(10000)\n",
    "    train_data, validation_and_test_data = train_test_split(data, test_size = 0.4)\n",
    "    validation_data, test_data = train_test_split(validation_and_test_data, test_size = 0.5)\n",
    "    return train_data.reset_index(), validation_data.reset_index(), test_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7bc8c2",
   "metadata": {},
   "source": [
    "### Dyskretyzacja danych\n",
    "Aby zapobiec nadmiernym dopasowaniu się drzewa dezycyjnego do danych trenujących dzielę je  \n",
    "na kategorie tj. dyskretyzuję. Każdą cechę, która tego potrzebuje dzielę na kilka przedziałów liczbowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4b16a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_data(df):\n",
    "    df = df.drop(['index', 'id'], axis=1)\n",
    "    df['age'] = pd.cut(x=df['age'], bins=8, labels=[\"1-10\", \"11-20\", \"21-30\", \"31-40\", \"41-50\", \"51-60\", \"61-70\", \"71-80\"])\n",
    "    df['height'] = pd.cut(x=df['height'], bins=[0, 155, 170, 185, 220], labels=['short', 'medium', 'high', 'higher'])\n",
    "    df['weight'] = pd.cut(x=df['weight'], bins=[0, 50, 60, 70, 80, 90, 100, 150, 400],\n",
    "                          labels=['light', '51-60', '61-70', '71-80','81-90','91-100','101-150','151-400'])\n",
    "    df['ap_hi'] = pd.cut(x=df['ap_hi'], bins=[0, 50, 90, 120, 150, 200], labels=['lower', 'low', 'medium', 'high', 'very_high'])\n",
    "    return df\n",
    "\n",
    "def split_and_disc(filename, deli):\n",
    "    data = split_data(filename, deli)\n",
    "    new_data = []\n",
    "    for data_set in data:\n",
    "        new_data.append(discretize_data(data_set))\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463401c2",
   "metadata": {},
   "source": [
    "### Implementacja interfejsu Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "339f41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self, max_depth, min_samples=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.decision_tree = None\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return {\"max_depth\" : self.max_depth, \"min_samples\" : self.min_samples}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        decision_tree = DecisionTree(self.max_depth)\n",
    "        train_data = pd.concat([X, y], axis=1)        \n",
    "        decision_tree.id3(train_data)\n",
    "        self.decision_tree = decision_tree\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.decision_tree == None: return\n",
    "        predictions = []\n",
    "        for instance in X:\n",
    "            predition = self.decision_tree.predict(decision_tree.tree, instance)\n",
    "            predictions.append(predition)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da1351",
   "metadata": {},
   "source": [
    "### Implementacja drzewa decyzyjnego\n",
    "- Główną funckją drzewa jest `make_tree`, które wywoływane jest rekurencyjnie, do momentu dojścia do maksymalnej głębokości  \n",
    "  lub gdy dane w *węźle* będą już tylko jednego typu. W przypadku niedokończenia drzewa przez ogranicznie `max_depth`,  \n",
    "  model wybiera najczęściej występującą klasę. Samo drzewo reprezentowane jest przez słownik.  \n",
    "- Funkcja `create_sub_tree` tworzy podrzewo na podstawie danej cechy i usuwa przypadki tej cechy z danych.\n",
    "- Funkcja `best_feature` zwraca cechę, na podstawie której będzie rosnąć drzewo w następnej kolejności tj. tą która ma  \n",
    "  największy przyrost informacyjny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10745ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth, min_samples=0):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.tree = None\n",
    "        \n",
    "    def id3(self, train_data):\n",
    "        label =  train_data.columns[-1]\n",
    "        tree = {}\n",
    "        unique_outputs = train_data[label].unique()\n",
    "        self.make_tree(tree, None, train_data, unique_outputs)\n",
    "        self.tree = tree\n",
    "    \n",
    "    def make_tree(self, root, prev_feature_value, train_data, unique_outputs, depth=0):\n",
    "        label =  train_data.columns[-1]\n",
    "        if train_data.shape[0] > self.min_samples:\n",
    "            feature = self.best_feature(train_data, unique_outputs)\n",
    "            tree, train_data = self.create_sub_tree(feature, train_data, unique_outputs)\n",
    "\n",
    "            if prev_feature_value != None:\n",
    "                root[prev_feature_value] = {}\n",
    "                root[prev_feature_value][feature] = tree\n",
    "                next_root = root[prev_feature_value][feature]\n",
    "            else:\n",
    "                root[feature] = tree\n",
    "                next_root = root[feature]\n",
    "\n",
    "            for node, branch in list(next_root.items()):\n",
    "                if depth > self.max_depth and train_data.shape[0] > self.min_samples: #check if depth in bounds\n",
    "                    next_root[node] = most_common(train_data)\n",
    "                    return\n",
    "                    \n",
    "                elif branch == None: #check if node isn't terminal\n",
    "                    feature_value_data = train_data[train_data[feature] == node]\n",
    "                    self.make_tree(next_root, node, feature_value_data, unique_outputs, depth+1)\n",
    "                    \n",
    "    def create_sub_tree(self, feature_name, train_data, unique_outputs):\n",
    "        label =  train_data.columns[-1]\n",
    "        feature_value_count_dict = train_data[feature_name].value_counts(sort=False)\n",
    "        tree = {}\n",
    "        \n",
    "        for fv, count in feature_value_count_dict.iteritems():\n",
    "            fv_data = train_data[train_data[feature_name] == fv]\n",
    "            assigned_to_node = False\n",
    "            for output in unique_outputs:\n",
    "                class_count = fv_data[fv_data[label] == output].shape[0]\n",
    "                if class_count == count:\n",
    "                    tree[fv] = output\n",
    "                    train_data = train_data[train_data[feature_name] != fv]\n",
    "                    assigned_to_node = True\n",
    "            if not assigned_to_node:\n",
    "                tree[fv] = None #unfinished node\n",
    "        return tree, train_data\n",
    "    \n",
    "    def best_feature(self, train_data, unique_outputs):\n",
    "        label =  train_data.columns[-1]\n",
    "        feature_list = train_data.columns.drop(label)\n",
    "        max_info_gain = -1\n",
    "        max_info_feature = None\n",
    "\n",
    "        for feature in feature_list:\n",
    "            feature_info_gain = inf_gain(feature, train_data, unique_outputs)\n",
    "            if max_info_gain < feature_info_gain:\n",
    "                max_info_gain = feature_info_gain\n",
    "                max_info_feature = feature\n",
    "        return max_info_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fdf9d6",
   "metadata": {},
   "source": [
    "### Utils - funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e9c5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_gain(feature_name, train_data, unique_outputs):\n",
    "    label =  train_data.columns[-1]\n",
    "    feature_value_list = train_data[feature_name].unique()\n",
    "    num_rows = train_data.shape[0]\n",
    "    feature_info = 0\n",
    "\n",
    "    for fv in feature_value_list:\n",
    "        fv_data = train_data[train_data[feature_name] == fv]\n",
    "        fv_count = fv_data.shape[0]\n",
    "        fv_entropy = feature_entropy(fv_data, unique_outputs)\n",
    "        fv_probability = fv_count / num_rows\n",
    "        feature_info += fv_probability * fv_entropy\n",
    "    return total_entropy(train_data, unique_outputs) - feature_info\n",
    "\n",
    "def total_entropy(train_data, unique_outputs):\n",
    "    label =  train_data.columns[-1]\n",
    "    num_rows = train_data.shape[0]\n",
    "    entropy = 0\n",
    "    for output in unique_outputs:\n",
    "        output_count = train_data[train_data[label] == output].shape[0]\n",
    "        output_entropy = - (output_count/num_rows) * np.log2(output_count / num_rows)\n",
    "        entropy += output_entropy\n",
    "    return entropy\n",
    "\n",
    "def feature_entropy(feature_value_data, unique_outputs):\n",
    "    label =  train_data.columns[-1]\n",
    "    output_count = feature_value_data.shape[0]\n",
    "    entropy = 0\n",
    "    for output in unique_outputs:\n",
    "        label_class_count = feature_value_data[feature_value_data[label] == output].shape[0] \n",
    "        entropy_class = 0\n",
    "        if label_class_count != 0:\n",
    "            probability_class = label_class_count / output_count\n",
    "            entropy_class = - probability_class * np.log2(probability_class)\n",
    "        entropy += entropy_class\n",
    "    return entropy\n",
    "\n",
    "def most_common(train_data):\n",
    "    label =  train_data.columns[-1]\n",
    "    labels = train_data[label]\n",
    "    unique, pos = np.unique(labels, return_inverse=True)\n",
    "    counts = np.bincount(pos)\n",
    "    maxpos = counts.argmax()\n",
    "\n",
    "    return unique[maxpos]\n",
    "\n",
    "def predict(tree, instance):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    else:\n",
    "        root_node = next(iter(tree))\n",
    "        feature_value = instance[root_node]\n",
    "        if feature_value in tree[root_node]:\n",
    "            return predict(tree[root_node][feature_value], instance)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def acuracy(tree, test_data):\n",
    "    label =  train_data.columns[-1]\n",
    "    correct = 0\n",
    "    wrong = 0\n",
    "    for index, _ in test_data.iterrows():\n",
    "        instance = test_data.iloc[index]\n",
    "        result = predict(tree, instance)\n",
    "        if result == test_data[label].iloc[index]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            wrong += 1\n",
    "    accuracy = correct / (correct + wrong)\n",
    "    return accuracy\n",
    "\n",
    "def precision_and_recall(tree, validation_data):\n",
    "    label =  train_data.columns[-1]\n",
    "    size = validation_data.shape[0]\n",
    "    res = {'TP':0, 'FP':0, 'FN':0, 'TN':0}\n",
    "    outputs = validation_data[label]\n",
    "\n",
    "    for index, _ in validation_data.iterrows():\n",
    "        instance = validation_data.iloc[index]\n",
    "        result = predict(tree, instance)\n",
    "        if result == 1 and outputs[index] == 1:\n",
    "             res['TP'] += 1\n",
    "        elif result == 1 and outputs[index] == 0:\n",
    "             res['FP'] += 1\n",
    "        elif result == 0 and outputs[index] == 1:\n",
    "            res['FN'] += 1\n",
    "        elif result == 0 and outputs[index] == 0:\n",
    "            res['TN'] += 1\n",
    "\n",
    "    precision = res['TP']/(res['TP'] + res['FP'])\n",
    "    recall = res['TP']/(res['TP'] + res['FN'])\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7f5ee",
   "metadata": {},
   "source": [
    "### Testy\n",
    "Postanowiono przetestować zbiór walidujący danych. Na początku zmniejszono liczbę próbek  \n",
    "danych do 10000 w celu przyśpieszenia czasu symulacji a następnie testowano model dla kilku  \n",
    "wartości maksymalnej głębokości przeszukiwania od 1 do 12. Jakość oceniono na postawie dokładności,  \n",
    "precyzji oraz pełności. Poniżej przedstawione zostały wyniki tych głębokości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b99c52a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxDepth = 1:\n",
      "    -> Accuracy = 27.7%\n",
      "    -> Precision = 51.32%\n",
      "    -> Recall = 54.78%\n",
      "\n",
      "MaxDepth = 2:\n",
      "    -> Accuracy = 28.9%\n",
      "    -> Precision = 62.16%\n",
      "    -> Recall = 59.47%\n",
      "\n",
      "MaxDepth = 3:\n",
      "    -> Accuracy = 39.4%\n",
      "    -> Precision = 66.34%\n",
      "    -> Recall = 70.59%\n",
      "\n",
      "MaxDepth = 4:\n",
      "    -> Accuracy = 50.2%\n",
      "    -> Precision = 66.67%\n",
      "    -> Recall = 67.18%\n",
      "\n",
      "MaxDepth = 5:\n",
      "    -> Accuracy = 52.35%\n",
      "    -> Precision = 65.29%\n",
      "    -> Recall = 60.92%\n",
      "\n",
      "MaxDepth = 6:\n",
      "    -> Accuracy = 51.6%\n",
      "    -> Precision = 64.1%\n",
      "    -> Recall = 63.38%\n",
      "\n",
      "MaxDepth = 7:\n",
      "    -> Accuracy = 55.35%\n",
      "    -> Precision = 65.24%\n",
      "    -> Recall = 62.41%\n",
      "\n",
      "MaxDepth = 8:\n",
      "    -> Accuracy = 49.6%\n",
      "    -> Precision = 65.79%\n",
      "    -> Recall = 66.12%\n",
      "\n",
      "MaxDepth = 9:\n",
      "    -> Accuracy = 45.45%\n",
      "    -> Precision = 65.75%\n",
      "    -> Recall = 67.79%\n",
      "\n",
      "MaxDepth = 10:\n",
      "    -> Accuracy = 40.35%\n",
      "    -> Precision = 65.42%\n",
      "    -> Recall = 70.71%\n",
      "\n",
      "MaxDepth = 11:\n",
      "    -> Accuracy = 40.35%\n",
      "    -> Precision = 65.42%\n",
      "    -> Recall = 70.71%\n",
      "\n",
      "MaxDepth = 12:\n",
      "    -> Accuracy = 40.35%\n",
      "    -> Precision = 65.42%\n",
      "    -> Recall = 70.71%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing max_depth parameter\n",
    "train_data, validation_data, test_data = split_and_disc(\"cardio_train.csv\", ';')\n",
    "\n",
    "for depth in range(1, 13):\n",
    "    solver = Solver(depth)\n",
    "    X = train_data.iloc[:,:-1]\n",
    "    y = train_data.iloc[:,-1]\n",
    "    \n",
    "    solver.fit(X, y)\n",
    "    decision_tree = solver.decision_tree\n",
    "    accuracy = acuracy(decision_tree.tree, validation_data)\n",
    "    precision, recall = precision_and_recall(decision_tree.tree, validation_data)\n",
    "    \n",
    "    print(f'MaxDepth = {depth}:\\n    -> Accuracy = {round(accuracy*100, 2)}%\\n\\\n",
    "    -> Precision = {round(precision*100, 2)}%\\n\\\n",
    "    -> Recall = {round(recall*100, 2)}%\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a76b2",
   "metadata": {},
   "source": [
    "### Wnioski z eksperymentu\n",
    "Z przeprowadzonych wyżej eksperymentów można wysnuć następujące wnioski:  \n",
    "\n",
    "Gdy `max_depth` przekroczy 7 oceny jakości przestają rosnąć, a nawet zaczynają maleć co widać miedzy przypadkami `max_depth 7->8`. Na ogół im większa maksymalna głębokość drzewa tym model jest dokładniejszy - jest o wiele bardziej  dopsasowany do danych trenujących i estymata wyjścia jest lepsza. Jednak przy zbyt dużych głębokościach obserwujemy  zmniejszenie ocen jakości. Dzieje  się tak, gdyż drzewa decyzyjne są bardzo podatne na przetrenowanie. To co się widzimy to  nadmierne dopasowanie się drzewa do  zbioru trenującego, które potem skutkuje marnymi wynikami na zbiorze walidacyjnym. Ponadto  wyniki mogą nie być najlepsze przez same dane lub dyskretyzację tych danych. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3d7a7",
   "metadata": {},
   "source": [
    "### Dobór max_depth\n",
    "W przypadku naszego problemu - wykrywanie choroby serca, bardziej zależy nam na pełności niż na precyzji, gdyż  \n",
    "chcemy zaminimalizować liczbę sytuacji, w których nie wykryjemy choroby serca u chorego pacjenta. W związku z tym,  \n",
    "ostatecznie dobrano wartość parametru `max_depth = 7`, ponieważ ten przypadek łączy w sobie prawie najlepsze możliwe  \n",
    "wyniki ocen jakości i nie zabiera dużo czasu.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2add93db",
   "metadata": {},
   "source": [
    "### Wnioski ogólne\n",
    "Skuteczność modelu drzewa decyzyjnego zależy od wielu czynników:  \n",
    "- samych danych.  \n",
    "- sposobie podziału zbioru danych na podzbiory treningowy, walidacyjny, testowy.  \n",
    "- doboru parametrów modelu przy użyciu zbioru walidacyjnego  \n",
    "- rozmiaru danych wejściowych - im więcej danych tym dokładniejszy model otrzymamy.  \n",
    "- liczby atrybutów zbioru danych - przy bardzo dużej ilości atrybutów wygenerowany model może działać niepoprawnie,  \n",
    "  lub jego  generacja moze trwać bardzo długo.\n",
    "- wadą jest jego długotrwałe wyliczanie \n",
    "- zaletą jest szybka klasyfikacja nowych danych na podstawie już raz wyliczonego modelu "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
