{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d144fb7",
   "metadata": {},
   "source": [
    "## WSI Ćwiczenie nr.6 - Algorytm Q-learning\n",
    "### Maciej Łodziński\n",
    "\n",
    "#### Cel ćwiczenia:\n",
    "Celem ćwiczenia jest implementacja algorytmu Q-learning oraz stworzenie agenta rozwiązującego problem `TAXI` z biblioteki `gym` i zbadanie wpływu poszczególnych hiperparametrów na działanie tego algorytmu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0180d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from random import uniform\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57060334",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee9b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, _env, _learn_rate, _discount, _exploration):\n",
    "        self.env = _env\n",
    "        self.learn_rate = _learn_rate\n",
    "        self.discount = _discount\n",
    "        self.exploration = _exploration\n",
    "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "\n",
    "    def q_learning(self, epochs):\n",
    "        for _ in range(epochs):\n",
    "            state = self.env.reset()\n",
    "            end = False\n",
    "\n",
    "            while not end:\n",
    "                random_value = uniform(0, 1)\n",
    "                if (random_value < self.exploration):\n",
    "                    action = self.env.action_space.sample() #exploration\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state]) #exploitation\n",
    "\n",
    "                next_state, reward, end, info = self.env.step(action)\n",
    "                curr_q = self.q_table[state, action]\n",
    "                next_max_q = np.max(self.q_table[next_state])\n",
    "                new_q =  curr_q + self.learn_rate * (reward + self.discount * next_max_q - curr_q)\n",
    "                self.q_table[state, action] = new_q\n",
    "                state = next_state\n",
    "\n",
    "    def make_trip(self, max_trips, is_visual=False):\n",
    "        state = self.env.reset()\n",
    "        end = False\n",
    "        trip_length = 0\n",
    "\n",
    "        while not end and trip_length < max_trips:\n",
    "            action = np.argmax(self.q_table[state])\n",
    "            next_state, reward, end, info = self.env.step(action)\n",
    "            state = next_state\n",
    "            trip_length += 1\n",
    "            if is_visual: self.env.render()\n",
    "        return trip_length\n",
    "\n",
    "    def average_trips(self, num_trips, max_trips):\n",
    "        lengths = []\n",
    "        for _ in range(num_trips):\n",
    "            length = self.make_trip(max_trips)\n",
    "            lengths.append(length)\n",
    "        return mean(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e009d",
   "metadata": {},
   "source": [
    "#### Ogólna funkcja do przeprowadzania eksperymentów\n",
    "Poniższa funkcja zwraca średnią długość przebytej drogi przy różnych tablicach Q otrzymanych z tych samych parametrów "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28c8f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_param(iterations, num_trips, max_trips, discount, learn_rate, exploration, epochs):    \n",
    "    result = 0\n",
    "    for _ in range(iterations):\n",
    "        agent = Agent(env, learn_rate, discount, exploration)\n",
    "        agent.q_learning(epochs)\n",
    "        result += agent.average_trips(num_trips, max_trips)\n",
    "    \n",
    "    result /= iterations\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123235b3",
   "metadata": {},
   "source": [
    "#### Dostosowanie parametru `discount` oraz zbadanie jego wpływu \n",
    "`Discount` określa jak bardzo brane pod uwagę mają być nagrody, które agent może otrzymać w przyszłości względem tych które otrzymuje w obecnym kroku. Im większa wartość parametru, tym przyszłe nagrody mają większą wagę. Z kolei w miarę zmniejszania tego parametru przyszłe nagrody jakie agent może otrzymać mają mniejszy wpływ, za to nagrody otrzymywane w danym kroku będą bardziej znaczące.  \n",
    "  \n",
    "Optymalną wartością w tym przypadku wydaje się być `discount = 0.9`. Ta wartość jest całkiem stała w obliczu przeprowadzania następnych eksperymentów, mimo elementu losowego w algorytmie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a378cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.35 18.41 16.75 16.27 14.47] -> best discount = 0.9\n"
     ]
    }
   ],
   "source": [
    "def test_discount(iterations, num_trips, max_trips):\n",
    "    discount = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    learn_rate = 0.3\n",
    "    exploration = 0.1\n",
    "    epochs = 1000\n",
    "    \n",
    "    num_tests = len(discount)\n",
    "    results = np.zeros(num_tests)\n",
    "    for i in range(num_tests):\n",
    "        test_result = test_param(iterations, num_trips, max_trips, discount[i], learn_rate, exploration, epochs)\n",
    "        results[i] = test_result\n",
    "  \n",
    "    results = np.around(results, 2)\n",
    "    best_param = discount[np.argmin(results)]\n",
    "    info = f'{results} -> best discount = {best_param}'\n",
    "    print(info)\n",
    "    \n",
    "test_discount(15, 10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50163c8",
   "metadata": {},
   "source": [
    "#### Dostosowanie parametru `learn_rate` oraz zbadanie jego wpływu\n",
    "`Learn_rate` określa jak szybko algorytm będzie zmieniał wartości w tablicy Q tj. jak ważne są nowo zdobyte informacje o środowisku względem tych które już posiada. Im większa wartość parametru, tym tablica Q będzie się szybciej zmieniać i na odwrót. Podobnie jak przy poprzednich projektach, zwiększenie jego wartości skutkuje szybszym uczeniem się agenta, i tym samym osiągnięciem satysfakcjonującego wyniku w mniejszej liczbie iteracji, lecz zbyt duża wartość parametru spowoduje, że agent będzie miał problemy ze znalezieniem rozwiązania.  \n",
    "  \n",
    "Optymalną wartością w tym przypadku wydaje się być `learn_rate = 0.7`, lecz zmienia się ona nieznacznie przy prowadzeniu kolejnych eksperymentów, gdyż mają one w sobie element losowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c335ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.54 13.83 13.01 13.59 13.65] -> best learn_rate = 0.7\n"
     ]
    }
   ],
   "source": [
    "def test_learn_rate(iterations, num_trips, max_trips):\n",
    "    discount = 0.9\n",
    "    learn_rate = [0.3, 0.5, 0.7, 0.9, 1.1]\n",
    "    exploration = 0.1\n",
    "    epochs = 1000\n",
    "    \n",
    "    num_tests = len(learn_rate)\n",
    "    results = np.zeros(num_tests)\n",
    "    for i in range(num_tests):\n",
    "        test_result = test_param(iterations, num_trips, max_trips, discount, learn_rate[i], exploration, epochs)\n",
    "        results[i] = test_result\n",
    "  \n",
    "    results = np.around(results, 2)\n",
    "    best_param = learn_rate[np.argmin(results)]\n",
    "    info = f'{results} -> best learn_rate = {best_param}'\n",
    "    print(info)\n",
    "    \n",
    "test_learn_rate(15, 10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aa7d15",
   "metadata": {},
   "source": [
    "#### Dostosowanie parametru `exploration` oraz zbadanie jego wpływu \n",
    "`Exploration` określa prawdopodobieństwo z jakim ma być wybrana losowa akcja spośród wszystkich dostępnych względem wyboru najlepszej akcji wynikającej z tablicy Q. Im większa wartość parametru, tym agent bardziej eksploruje ( wybiera akcję losowo i tym samym poznaje środowsko), natomiast mała wartość będzie oznaczać eksploatację - podążeanie względem strategii zapisanej w tablicy Q.  \n",
    "  \n",
    "Optymalną wartością w tym przypadku wydaje się być `exploration = 0.15`. Mała wartość tego parametru ma sens, gdyż chcemy eksplorować tylko raz na jakiś czas, ale przeważnie wybierać jednak najlepszą akcję z tablicy Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c25f62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.69 13.61 13.31 13.97] -> best exploration = 0.15\n"
     ]
    }
   ],
   "source": [
    "def test_exploration(iterations, num_trips, max_trips):\n",
    "    discount = 0.9\n",
    "    learn_rate = 0.7\n",
    "    exploration = [0.05, 0.1 , 0.15, 0.2]\n",
    "    epochs = 1000\n",
    "    \n",
    "    num_tests = len(exploration)\n",
    "    results = np.zeros(num_tests)\n",
    "    for i in range(num_tests):\n",
    "        test_result = test_param(iterations, num_trips, max_trips, discount, learn_rate, exploration[i], epochs)\n",
    "        results[i] = test_result\n",
    "  \n",
    "    results = np.around(results, 2)\n",
    "    best_param = exploration[np.argmin(results)]\n",
    "    info = f'{results} -> best exploration = {best_param}'\n",
    "    print(info)\n",
    "    \n",
    "test_exploration(15, 10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91acf8ed",
   "metadata": {},
   "source": [
    "#### Ostateczny test z dobranymi parametrami\n",
    "`learn_rate` = 0.7  \n",
    "`discount` = 0.9  \n",
    "`exploration` = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae4fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "trip lenght = 16\n"
     ]
    }
   ],
   "source": [
    "def final_test(epochs, max_trips):\n",
    "    agent = Agent(_env=env, _learn_rate=0.7, _discount=0.9, _exploration=0.15)\n",
    "    agent.q_learning(epochs)\n",
    "    trip_length = agent.make_trip(max_trips, is_visual=True)\n",
    "    \n",
    "    if trip_length >= max_trips:\n",
    "        print(\"\\nPath not found!\")\n",
    "    else:\n",
    "        print(f'\\ntrip lenght = {trip_length}')\n",
    "    \n",
    "final_test(epochs=1000, max_trips=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9ba14",
   "metadata": {},
   "source": [
    "### Wnioski:\n",
    "Po dobraniu odpowiednich parametrów i przeprowadzenia eksperymentu można stwierdzić że algorytm Q learning świetnie sobie radzi z poznawaniem i nauką nieznanego wcześniej środowska. Po nauczeniu agenta, przejeżdża on teraz trasę w najkrótszym możliwym czasie."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
